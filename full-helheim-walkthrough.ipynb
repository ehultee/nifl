{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing spatial pattern of velocity response to forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the `iceutils` package (Bryan Riel) to invert continuous time-varying surface velocity fields on Helheim Glacier.  We'll then process several observational datasets (gathered by Denis Felikson) using the `nifl` module (Lizz Ultee) and compare time series of these variables against surface velocity at several points.  Finally, we'll visualize spatial differences in the relationship between surface velocity and each hypothesised forcing variable.\n",
    "\n",
    "Last updated: 6 Jan 2022 by Lizz Ultee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "from scipy import interpolate\n",
    "import pyproj as pyproj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LightSource\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable \n",
    "import iceutils as ice\n",
    "import nifl_helper as nifl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define where the necessary data lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowline_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Felikson-flowlines/netcdfs/glaciera199.nc'\n",
    "velocity_fpath='/Users/lizz/Documents/GitHub/Data_unsynced/Gld-Stack/'\n",
    "gl_bed_fpath ='/Users/lizz/Documents/GitHub/Data_unsynced/BedMachine-Greenland/BedMachineGreenland-2017-09-20.nc'\n",
    "catchment_smb_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Helheim-processed/smb_rec._.BN_RACMO2.3p2_ERA5_3h_FGRN055.1km.MM.csv'\n",
    "runoff_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Helheim-processed/runoff._.BN_RACMO2.3p2_ERA5_3h_FGRN055.1km.MM.csv'\n",
    "termini_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Helheim-processed/HLM_terminus_widthAVE.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the domain of analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse along flowlines defined by Denis Felikson in his previous work, saved and shared as NetCDF files.  The flowlines are numbered 01-10 across the terminus; flowline 05 is close to the middle.  Note that Helheim Glacier has two large branches.  For now we'll study the main trunk, `glaciera199.nc`.  The more southerly trunk is `glacierb199.nc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncfile = Dataset(flowline_fpath, 'r')\n",
    "xh = ncfile['flowline05'].variables['x'][:]\n",
    "yh = ncfile['flowline05'].variables['y'][:]\n",
    "ncfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define points at which to extract\n",
    "upstream_max = 500 # index of last xh,yh within given distance of terminus--pts roughly 50m apart\n",
    "xys = [(xh[i], yh[i]) for i in range(0, upstream_max, 20)][2::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and invert velocity observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up combined hdf5 stack\n",
    "hel_stack = ice.MagStack(files=[velocity_fpath+'vx.h5', velocity_fpath+'vy.h5'])\n",
    "data_key = 'igram' # B. Riel convention for access to datasets in hdf5 stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evenly spaced time array for time series predictions\n",
    "t_grid = np.linspace(hel_stack.tdec[0], hel_stack.tdec[-1], 1000)\n",
    "\n",
    "# First convert the time vectors to a list of datetime\n",
    "dates = ice.tdec2datestr(hel_stack.tdec, returndate=True)\n",
    "dates_grid = ice.tdec2datestr(t_grid, returndate=True)\n",
    "\n",
    "# Build the collection\n",
    "collection = nifl.build_collection(dates)\n",
    "\n",
    "# Construct a priori covariance\n",
    "Cm = nifl.computeCm(collection)\n",
    "iCm = np.linalg.inv(Cm)\n",
    "\n",
    "# Instantiate a model for inversion\n",
    "model = ice.tseries.Model(dates, collection=collection)\n",
    "\n",
    "# Instantiate a model for prediction\n",
    "model_pred = ice.tseries.Model(dates_grid, collection=collection)\n",
    "\n",
    "## Access the design matrix for plotting\n",
    "G = model.G\n",
    "\n",
    "# Create lasso regression solver that does the following:\n",
    "# i) Uses an a priori covariance matrix for damping out the B-splines\n",
    "# ii) Uses sparsity-enforcing regularization (lasso) on the integrated B-splines\n",
    "solver = ice.tseries.select_solver('lasso', reg_indices=model.itransient, penalty=0.05,\n",
    "                                   rw_iter=1, regMat=iCm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are set up with our data and machinery, we'll ask the inversion to make us a continuous time series of velocity at each point we wish to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for j, xy in enumerate(xys):\n",
    "    try:\n",
    "        pred, st, lt = nifl.VSeriesAtPoint(xy, vel_stack=hel_stack, collection=collection, \n",
    "                                  model=model, model_pred=model_pred, solver=solver, \n",
    "                                  t_grid=t_grid, sigma=2.5, data_key='igram')\n",
    "        preds.append(pred)\n",
    "    except AssertionError: # catches failed inversion\n",
    "        print('Insufficient data for point {}. Removing'.format(j))\n",
    "        xys.remove(xy)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to confirm that `xys` has been trimmed to only those points for which a valid prediction can be generated -- the below should return `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xys)==len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bed topography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly we will use this for plotting and for defining a standard coordinate system.  However, future analyses could combine bed topography with calving position or other variables to analyse effect on surface velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in and interpolate BedMachine topography\n",
    "fh = Dataset(gl_bed_fpath, mode='r')\n",
    "xx = fh.variables['x'][:].copy() #x-coord (polar stereo (70, 45))\n",
    "yy = fh.variables['y'][:].copy() #y-coord\n",
    "s_raw = fh.variables['surface'][:].copy() #surface elevation\n",
    "h_raw=fh.variables['thickness'][:].copy() # Gridded thickness\n",
    "b_raw = fh.variables['bed'][:].copy() # bed topo\n",
    "thick_mask = fh.variables['mask'][:].copy()\n",
    "ss = np.ma.masked_where(thick_mask !=2, s_raw)#mask values: 0=ocean, 1=ice-free land, 2=grounded ice, 3=floating ice, 4=non-Greenland land\n",
    "hh = np.ma.masked_where(thick_mask !=2, h_raw) \n",
    "bb = b_raw #don't mask, to allow bed sampling from modern bathymetry (was subglacial in ~2006)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpolate in area of Helheim\n",
    "xl, xr = 6100, 6600\n",
    "yt, yb = 12700, 13100\n",
    "x_hel = xx[xl:xr]\n",
    "y_hel = yy[yt:yb]\n",
    "s_hel = ss[yt:yb, xl:xr]\n",
    "b_hel = bb[yt:yb, xl:xr]\n",
    "S_helheim = interpolate.RectBivariateSpline(x_hel, y_hel[::-1], s_hel.T[::,::-1]) #interpolating surface elevation provided\n",
    "B_helheim = interpolate.RectBivariateSpline(x_hel, y_hel[::-1], b_hel.T[::,::-1]) #interpolating bed elevation provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catchment-integrated SMB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in a 1D timeseries of surface mass balance integrated over the whole Helheim catchment.  This data is monthly surface mass balance from the HIRHAM5 model, integrated over the Helheim catchment defined by K. Mankoff, with processing steps (coordinate reprojection, Delaunay triangulation, nearest-neighbor search and area summing) in `catchment-integrate-smb.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smb_racmo = pd.read_csv(catchment_smb_fpath, index_col=0, parse_dates=True)\n",
    "smb_tr = smb_racmo.loc[smb_racmo.index.year >= 2006]\n",
    "smb = smb_tr.loc[smb_tr.index.year <2018].squeeze() # trim dates to overlapping period\n",
    "\n",
    "smb_d = [d.utctimetuple() for d in smb.index]\n",
    "smb_d_interp = [ice.timeutils.datestr2tdec(d[0], d[1], d[2]) for d in smb_d]\n",
    "smb_func = interpolate.interp1d(smb_d_interp, smb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series we analyse here are autocorrelated. We must compute a correction factor to the significance limits based on the lag-1 autocorrelation, as described in Dean & Dunsmuir (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_vel = sm.tsa.stattools.acf(np.diff(preds[0]['full']))[1]\n",
    "b_smb = sm.tsa.stattools.acf(np.diff(smb))[1]\n",
    "F_smb = np.sqrt((1+(a_vel*b_smb))/(1-(a_vel*b_smb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the normalized cross-correlation between catchment-integrated SMB and surface velocity at each point along the flowline.  We will draw on the inverted velocity series saved in `preds` above.  We save the value of the maximum normalized cross-correlation, and the value in days of the lag where it occurs, to compare with other variables later.  We also test whether each saved value is significantly different from 0, with the confidence interval around 0 modified as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smb_corr_amax = []\n",
    "smb_lag_amax = []\n",
    "smb_significance = []\n",
    "\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = nifl.Xcorr1D(xy, series_func=smb_func, series_dates=smb_d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, t_limits=(2009,2017), \n",
    "                              diff=1, normalize=True, pos_only=True)\n",
    "    ci_mod = F_smb*np.asarray(ci)\n",
    "    smb_corr_amax.append(corr[abs(corr).argmax()])\n",
    "    smb_lag_amax.append(lags[abs(corr).argmax()])\n",
    "    smb_significance.append(abs(corr[abs(corr).argmax()]) > ci_mod[abs(corr).argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import monthly runoff from the RACMO model, integrated over the Helheim catchment and shared as a CSV by Denis Felikson.  Because this data is catchment-integrated, we interpolate a single 1D time series that will be used at all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff_racmo = pd.read_csv(runoff_fpath, index_col=0, parse_dates=True)\n",
    "runoff_tr = runoff_racmo.loc[runoff_racmo.index.year >= 2006]\n",
    "runoff = runoff_tr.loc[runoff_tr.index.year <2018].squeeze()\n",
    "\n",
    "runoff_d = [d.utctimetuple() for d in runoff.index]\n",
    "d_interp = [ice.timeutils.datestr2tdec(d[0], d[1], d[2]) for d in runoff_d]\n",
    "runoff_func = interpolate.interp1d(d_interp, runoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we compute a correction factor to account for autocorrelated data in the 95% confidence interval around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_runoff = sm.tsa.stattools.acf(np.diff(runoff))[1]\n",
    "F_runoff = np.sqrt((1+(a_vel*b_runoff))/(1-(a_vel*b_runoff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the normalized cross-correlation between catchment-integrated runoff and surface velocity at each same point.  Again we save the value of the maximum normalized cross-correlation, and the value in days of the lag where it occurs, to compare with other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff_corr_amax = []\n",
    "runoff_lag_amax = []\n",
    "runoff_significance = []\n",
    "\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = nifl.Xcorr1D(xy, series_func=runoff_func, series_dates=d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, t_limits=(2009,2017), \n",
    "                              diff=1, normalize=True, pos_only=True)\n",
    "    ci_mod = F_runoff*np.asarray(ci)\n",
    "    runoff_corr_amax.append(corr[abs(corr).argmax()])\n",
    "    runoff_lag_amax.append(lags[abs(corr).argmax()])\n",
    "    runoff_significance.append(abs(corr[abs(corr).argmax()]) > ci_mod[abs(corr).argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminus position change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import width-averaged terminus position change processed by Leigh Stearns.  These data give terminus position in km from a baseline, so they do not need to be processed into a coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termini = pd.read_csv(termini_fpath, index_col=0, parse_dates=True, usecols=[0,1])\n",
    "trmn = termini.loc[termini.index.year >= 2006]\n",
    "tm = trmn.loc[trmn.index.year <2017].squeeze()\n",
    "\n",
    "## smooth a little to make more comparable with SMB and runoff\n",
    "td = tm.rolling('10D').mean() # approximately 3 measurements per window\n",
    "\n",
    "termini_d = [d.utctimetuple() for d in td.index]\n",
    "tm_d_interp = [ice.timeutils.datestr2tdec(d[0], d[1], d[2]) for d in termini_d]\n",
    "termini_func = interpolate.interp1d(tm_d_interp, td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the confidence interval correction factor for these data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_terminus = sm.tsa.stattools.acf(np.diff(td))[1]\n",
    "F_terminus = np.sqrt((1+(a_vel*b_terminus))/(1-(a_vel*b_terminus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, finding the maximum cross-correlation and the (positive) lag at which it occurs, as for other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminus_corr_amax = []\n",
    "terminus_lag_amax = []\n",
    "terminus_significance = []\n",
    "\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = nifl.Xcorr1D(xy, series_func=termini_func, series_dates=tm_d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, t_limits=(2009,2017), \n",
    "                              diff=1, normalize=True, pos_only=True)\n",
    "    ci_mod = F_terminus *np.asarray(ci)\n",
    "    terminus_corr_amax.append(corr[abs(corr).argmax()])\n",
    "    terminus_lag_amax.append(lags[abs(corr).argmax()])\n",
    "    terminus_significance.append(abs(corr[abs(corr).argmax()]) > ci_mod[abs(corr).argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial pattern of cross-correlation (Figure 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the patterns of cross-correlation for each variable, marking whether each value is significant.  Here we use a filled dot to indicate values significantly different from 0, and a cross to indicate values not significantly different from 0.  We will produce Figure 2 of the Ultee et al manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_colors = 'RdBu' # choose divergent colormap for xcorr\n",
    "corrnorm_min, corrnorm_max = -0.3, 0.3\n",
    "lag_colors = 'Greens'\n",
    "lagnorm_min, lagnorm_max = 0, 365\n",
    "\n",
    "sig_markers = ['o', 'x']\n",
    "\n",
    "ls = LightSource(azdeg=225, altdeg=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set matplotlib font size defaults\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## black-white hillshade topo underneath\n",
    "rgb2 = ls.shade(np.asarray(b_hel), cmap=plt.get_cmap('gray'), blend_mode='overlay',\n",
    "               dx=np.mean(np.diff(x_hel)), dy=np.mean(np.diff(y_hel)), vert_exag=5.)\n",
    "\n",
    "fig, ((ax1, ax2, ax3), (ax4,ax5,ax6)) = plt.subplots(nrows=2,ncols=3, figsize=(12, 8), \n",
    "                                                     # constrained_layout=True, \n",
    "                                                     sharex=True, sharey=True,\n",
    "                                                     gridspec_kw={'wspace':0.01})\n",
    "    \n",
    "ax1.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc1 = ax1.scatter(np.asarray(xys)[smb_significance,0], np.asarray(xys)[smb_significance,1], \n",
    "                  c=np.asarray(smb_corr_amax)[smb_significance], cmap=div_colors, marker=sig_markers[0], \n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "ax1.scatter(np.asarray(xys)[np.invert(smb_significance),0], np.asarray(xys)[np.invert(smb_significance),1], \n",
    "                  c=np.asarray(smb_corr_amax)[np.invert(smb_significance)], cmap=div_colors, marker=sig_markers[1], \n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max) #different marker for insig values\n",
    "ax1.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "        ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "       ylabel='Northing [km]', title='Catchment SMB')\n",
    "\n",
    "ax2.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc2 = ax2.scatter(np.asarray(xys)[runoff_significance,0], np.asarray(xys)[runoff_significance,1], \n",
    "                  c=np.asarray(runoff_corr_amax)[runoff_significance], cmap=div_colors, marker=sig_markers[0],\n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "ax2.scatter(np.asarray(xys)[np.invert(runoff_significance),0], np.asarray(xys)[np.invert(runoff_significance),1],\n",
    "            c=np.asarray(runoff_corr_amax)[np.invert(runoff_significance)], cmap=div_colors, marker=sig_markers[1],\n",
    "            vmin=corrnorm_min, vmax=corrnorm_max) # distinguish insig values\n",
    "ax2.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "       title='Catchment runoff')\n",
    "\n",
    "ax3.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc3 = ax3.scatter(np.asarray(xys)[terminus_significance,0], np.asarray(xys)[terminus_significance,1], \n",
    "                  c=np.asarray(terminus_corr_amax)[terminus_significance], cmap=div_colors, marker=sig_markers[0],\n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "ax3.scatter(np.asarray(xys)[np.invert(terminus_significance),0], np.asarray(xys)[np.invert(terminus_significance),1],\n",
    "            c=np.asarray(terminus_corr_amax)[np.invert(terminus_significance)], cmap=div_colors, marker=sig_markers[1],\n",
    "            vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "\n",
    "div3 = make_axes_locatable(ax3)\n",
    "cax3 = div3.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cb3 = fig.colorbar(sc3, cax=cax3)\n",
    "cb3.ax.set_ylabel('AMax. xcorr')\n",
    "ax3.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "       title='Terminus position', aspect=1.)\n",
    "\n",
    "## SECOND ROW\n",
    "ax4.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc4 = ax4.scatter(np.asarray(xys)[smb_significance,0], np.asarray(xys)[smb_significance,1], \n",
    "                  c=np.asarray(smb_lag_amax)[smb_significance], cmap=lag_colors, marker=sig_markers[0],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax4.scatter(np.asarray(xys)[np.invert(smb_significance),0], np.asarray(xys)[np.invert(smb_significance),1], \n",
    "            c=np.asarray(smb_lag_amax)[np.invert(smb_significance)], cmap=lag_colors, marker=sig_markers[1],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax4.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', ylabel='Northing [km]')\n",
    "\n",
    "ax5.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc5 = ax5.scatter(np.asarray(xys)[runoff_significance,0], np.asarray(xys)[runoff_significance,1], \n",
    "                  c=np.asarray(runoff_lag_amax)[runoff_significance], cmap=lag_colors, marker=sig_markers[0],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax5.scatter(np.asarray(xys)[np.invert(runoff_significance),0], np.asarray(xys)[np.invert(runoff_significance),1], \n",
    "            c=np.asarray(runoff_lag_amax)[np.invert(runoff_significance)], cmap=lag_colors, marker=sig_markers[1],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax5.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]')\n",
    "\n",
    "ax6.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc6 = ax6.scatter(np.asarray(xys)[terminus_significance,0], np.asarray(xys)[terminus_significance,1], \n",
    "                  c=np.asarray(terminus_lag_amax)[terminus_significance], cmap=lag_colors, marker=sig_markers[0],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax6.scatter(np.asarray(xys)[np.invert(terminus_significance),0], np.asarray(xys)[np.invert(terminus_significance),1], \n",
    "            c=np.asarray(terminus_lag_amax)[np.invert(terminus_significance)], cmap=lag_colors, marker=sig_markers[1],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "\n",
    "div6 = make_axes_locatable(ax6)\n",
    "cax6 = div6.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cb6 = fig.colorbar(sc6, cax=cax6)\n",
    "cb6.ax.set_ylabel('Lag [d] at peak xcorr')\n",
    "cb6.set_ticks([0, 60, 120, 180, 240, 300, 360])\n",
    "ax6.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', aspect=1.)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# plt.savefig('/Users/lizz/Desktop/20210204-helheim-xcorr_lag_composite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual chunks to compare changing seasonal cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break signals into annual subsets and compute the cross-correlation signal for each single year of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_annual_corrs = []\n",
    "rf_annual_lags = []\n",
    "rf_annual_ci = []\n",
    "\n",
    "point_to_plot =5\n",
    "date_chks = range(2009, 2017)\n",
    "for i in range(len(date_chks)-1):\n",
    "#     snippet = rf[rf[:,0]>=date_chks[i]]\n",
    "#     snpt = snippet[snippet[:,0]<date_chks[i+1]]\n",
    "#     d_chk = [d for d in d_interp if (d>=date_chks[i] and d<=date_chks[i+1])]\n",
    "    corr, lags, ci = nifl.Xcorr1D(xys[point_to_plot], series_func=runoff_func, series_dates=d_interp, \n",
    "                              velocity_pred=preds[point_to_plot], t_grid=t_grid, t_limits=(date_chks[i], date_chks[i+1]),\n",
    "                                  diff=1, normalize=True)\n",
    "    rf_annual_corrs.append(corr)\n",
    "    rf_annual_lags.append(lags)\n",
    "    rf_annual_ci.append(ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(len(rf_annual_corrs))\n",
    "# for j in range(len(rf_annual_corrs)):\n",
    "#     axs[j].plot(rf_annual_lags[j], rf_annual_corrs[j])\n",
    "#     axs[j].plot(rf_annual_lags[j], rf_annual_ci[j], ls=':', color='k')\n",
    "#     axs[j].plot(rf_annual_lags[j], -1*np.array(rf_annual_ci[j]), ls=':', color='k')\n",
    "\n",
    "for j in range(len(rf_annual_corrs)):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.axvline(x=0, ls='-', color='k', alpha=0.5)\n",
    "    ax.axhline(y=0, ls='-', color='k', alpha=0.5)\n",
    "    ax.plot(rf_annual_lags[j], rf_annual_corrs[j])\n",
    "    ax.plot(rf_annual_lags[j], rf_annual_ci[j], ls=':', color='k')\n",
    "    ax.plot(rf_annual_lags[j], -1*np.array(rf_annual_ci[j]), ls=':', color='k')\n",
    "    ax.set(ylim=(-1,1), title='Xcorr runoff-vel, {}'.format(date_chks[j]), \n",
    "           xlabel='Lag [days]', ylabel='xcorr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with the overall signal from the full period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, lags, ci = nifl.Xcorr1D(xys[5], series_func=runoff_func, series_dates=d_interp, \n",
    "                          velocity_pred=preds[5], t_grid=t_grid, t_limits=(2009,2017), diff=1, normalize=True)\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.axvline(x=0, ls='-', color='k', alpha=0.5)\n",
    "ax.axhline(y=0, ls='-', color='k', alpha=0.5)\n",
    "ax.plot(lags, corr)\n",
    "ax.plot(lags, ci, ls=':', color='k')\n",
    "ax.plot(lags, -1*np.array(ci), ls=':', color='k')\n",
    "ax.set(ylim=(-1,1), title='Xcorr runoff-vel, 2009-2017', xlabel='Lag [days]', ylabel='xcorr');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
