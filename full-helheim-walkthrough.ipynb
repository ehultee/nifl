{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing spatial pattern of velocity response to forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the `iceutils` package (Bryan Riel) to invert continuous time-varying surface velocity fields on Helheim Glacier.  We'll then process several observational datasets (gathered by Denis Felikson) using the `nifl` module (Lizz Ultee) and compare time series of these variables against surface velocity at several points.  Finally, we'll visualize spatial differences in the relationship between surface velocity and each hypothesised forcing variable.\n",
    "\n",
    "Last updated: 6 Jan 2022 by Lizz Ultee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "from scipy import interpolate, ndimage\n",
    "import pyproj as pyproj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker, cm\n",
    "from matplotlib.colors import LightSource\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable \n",
    "import iceutils as ice\n",
    "import nifl_helper as nifl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define where the necessary data lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowline_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Felikson-flowlines/netcdfs/glaciera199.nc'\n",
    "velocity_fpath='/Volumes/Backup Plus/Large data moved 20220331/Gld-Stack/'\n",
    "gl_bed_fpath ='/Users/lizz/Documents/GitHub/Data_unsynced/BedMachine-Greenland/BedMachineGreenland-2017-09-20.nc'\n",
    "catchment_smb_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Helheim-processed/smb_rec._.BN_RACMO2.3p2_ERA5_3h_FGRN055.1km.MM.csv'\n",
    "runoff_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Helheim-processed/runoff._.BN_RACMO2.3p2_ERA5_3h_FGRN055.1km.MM.csv'\n",
    "termini_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Helheim-processed/HLM_terminus_widthAVE.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the domain of analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse along flowlines defined by Denis Felikson in his previous work, saved and shared as NetCDF files.  The flowlines are numbered 01-10 across the terminus; flowline 05 is close to the middle.  Note that Helheim Glacier has two large branches.  For now we'll study the main trunk, `glaciera199.nc`.  The more southerly trunk is `glacierb199.nc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncfile = Dataset(flowline_fpath, 'r')\n",
    "xh = ncfile['flowline05'].variables['x'][:]\n",
    "yh = ncfile['flowline05'].variables['y'][:]\n",
    "ncfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define points at which to extract\n",
    "upstream_max = 500 # index of last xh,yh within given distance of terminus--pts roughly 50m apart\n",
    "xys = [(xh[i], yh[i]) for i in range(0, upstream_max, 20)][2::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and invert velocity observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up combined hdf5 stack\n",
    "hel_stack = ice.MagStack(files=[velocity_fpath+'vx.h5', velocity_fpath+'vy.h5'])\n",
    "data_key = 'igram' # B. Riel convention for access to datasets in hdf5 stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evenly spaced time array for time series predictions\n",
    "t_grid = np.linspace(hel_stack.tdec[0], hel_stack.tdec[-1], 1000)\n",
    "\n",
    "# First convert the time vectors to a list of datetime\n",
    "dates = ice.tdec2datestr(hel_stack.tdec, returndate=True)\n",
    "dates_grid = ice.tdec2datestr(t_grid, returndate=True)\n",
    "\n",
    "# Build the collection\n",
    "collection = nifl.build_collection(dates)\n",
    "\n",
    "# Construct a priori covariance\n",
    "Cm = nifl.computeCm(collection)\n",
    "iCm = np.linalg.inv(Cm)\n",
    "\n",
    "# Instantiate a model for inversion\n",
    "model = ice.tseries.Model(dates, collection=collection)\n",
    "\n",
    "# Instantiate a model for prediction\n",
    "model_pred = ice.tseries.Model(dates_grid, collection=collection)\n",
    "\n",
    "## Access the design matrix for plotting\n",
    "G = model.G\n",
    "\n",
    "# Create lasso regression solver that does the following:\n",
    "# i) Uses an a priori covariance matrix for damping out the B-splines\n",
    "# ii) Uses sparsity-enforcing regularization (lasso) on the integrated B-splines\n",
    "solver = ice.tseries.select_solver('lasso', reg_indices=model.itransient, penalty=0.05,\n",
    "                                   rw_iter=1, regMat=iCm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are set up with our data and machinery, we'll ask the inversion to make us a continuous time series of velocity at each point we wish to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for j, xy in enumerate(xys):\n",
    "    try:\n",
    "        pred, st, lt = nifl.VSeriesAtPoint(xy, vel_stack=hel_stack, collection=collection, \n",
    "                                  model=model, model_pred=model_pred, solver=solver, \n",
    "                                  t_grid=t_grid, sigma=2.5, data_key='igram')\n",
    "        preds.append(pred)\n",
    "    except AssertionError: # catches failed inversion\n",
    "        print('Insufficient data for point {}. Removing'.format(j))\n",
    "        xys.remove(xy)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to confirm that `xys` has been trimmed to only those points for which a valid prediction can be generated -- the below should return `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xys)==len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bed topography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly we will use this for plotting and for defining a standard coordinate system.  However, future analyses could combine bed topography with calving position or other variables to analyse effect on surface velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in and interpolate BedMachine topography\n",
    "fh = Dataset(gl_bed_fpath, mode='r')\n",
    "xx = fh.variables['x'][:].copy() #x-coord (polar stereo (70, 45))\n",
    "yy = fh.variables['y'][:].copy() #y-coord\n",
    "s_raw = fh.variables['surface'][:].copy() #surface elevation\n",
    "h_raw=fh.variables['thickness'][:].copy() # Gridded thickness\n",
    "b_raw = fh.variables['bed'][:].copy() # bed topo\n",
    "thick_mask = fh.variables['mask'][:].copy()\n",
    "ss = np.ma.masked_where(thick_mask !=2, s_raw)#mask values: 0=ocean, 1=ice-free land, 2=grounded ice, 3=floating ice, 4=non-Greenland land\n",
    "hh = np.ma.masked_where(thick_mask !=2, h_raw) \n",
    "bb = b_raw #don't mask, to allow bed sampling from modern bathymetry (was subglacial in ~2006)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpolate in area of Helheim\n",
    "xl, xr = 6100, 6600\n",
    "yt, yb = 12700, 13100\n",
    "x_hel = xx[xl:xr]\n",
    "y_hel = yy[yt:yb]\n",
    "s_hel = ss[yt:yb, xl:xr]\n",
    "b_hel = bb[yt:yb, xl:xr]\n",
    "S_helheim = interpolate.RectBivariateSpline(x_hel, y_hel[::-1], s_hel.T[::,::-1]) #interpolating surface elevation provided\n",
    "B_helheim = interpolate.RectBivariateSpline(x_hel, y_hel[::-1], b_hel.T[::,::-1]) #interpolating bed elevation provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catchment-integrated SMB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in a 1D timeseries of surface mass balance integrated over the whole Helheim catchment.  This data is monthly surface mass balance from the HIRHAM5 model, integrated over the Helheim catchment defined by K. Mankoff, with processing steps (coordinate reprojection, Delaunay triangulation, nearest-neighbor search and area summing) in `catchment-integrate-smb.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smb_racmo = pd.read_csv(catchment_smb_fpath, index_col=0, parse_dates=True)\n",
    "smb_tr = smb_racmo.loc[smb_racmo.index.year >= 2006]\n",
    "smb = smb_tr.loc[smb_tr.index.year <2018].squeeze() # trim dates to overlapping period\n",
    "\n",
    "smb_d = [d.utctimetuple() for d in smb.index]\n",
    "smb_d_interp = [ice.timeutils.datestr2tdec(d[0], d[1], d[2]) for d in smb_d]\n",
    "smb_func = interpolate.interp1d(smb_d_interp, smb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series we analyse here are autocorrelated. We must compute a correction factor to the significance limits based on the lag-1 autocorrelation, as described in Dean & Dunsmuir (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_vel = sm.tsa.stattools.acf(np.diff(preds[0]['full']))[1]\n",
    "b_smb = sm.tsa.stattools.acf(np.diff(smb))[1]\n",
    "F_smb = np.sqrt((1+(a_vel*b_smb))/(1-(a_vel*b_smb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the normalized cross-correlation between catchment-integrated SMB and surface velocity at each point along the flowline.  We will draw on the inverted velocity series saved in `preds` above.  We save the value of the maximum normalized cross-correlation, and the value in days of the lag where it occurs, to compare with other variables later.  We also test whether each saved value is significantly different from 0, with the confidence interval around 0 modified as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smb_corr_amax = []\n",
    "smb_lag_amax = []\n",
    "smb_significance = []\n",
    "\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = nifl.Xcorr1D(xy, series_func=smb_func, series_dates=smb_d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, t_limits=(2009,2017), \n",
    "                              diff=1, normalize=True, pos_only=True)\n",
    "    ci_mod = F_smb*np.asarray(ci)\n",
    "    smb_corr_amax.append(corr[abs(corr).argmax()])\n",
    "    smb_lag_amax.append(lags[abs(corr).argmax()])\n",
    "    smb_significance.append(abs(corr[abs(corr).argmax()]) > ci_mod[abs(corr).argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import monthly runoff from the RACMO model, integrated over the Helheim catchment and shared as a CSV by Denis Felikson.  Because this data is catchment-integrated, we interpolate a single 1D time series that will be used at all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff_racmo = pd.read_csv(runoff_fpath, index_col=0, parse_dates=True)\n",
    "runoff_tr = runoff_racmo.loc[runoff_racmo.index.year >= 2006]\n",
    "runoff = runoff_tr.loc[runoff_tr.index.year <2018].squeeze()\n",
    "\n",
    "runoff_d = [d.utctimetuple() for d in runoff.index]\n",
    "d_interp = [ice.timeutils.datestr2tdec(d[0], d[1], d[2]) for d in runoff_d]\n",
    "runoff_func = interpolate.interp1d(d_interp, runoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we compute a correction factor to account for autocorrelated data in the 95% confidence interval around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_runoff = sm.tsa.stattools.acf(np.diff(runoff))[1]\n",
    "F_runoff = np.sqrt((1+(a_vel*b_runoff))/(1-(a_vel*b_runoff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the normalized cross-correlation between catchment-integrated runoff and surface velocity at each same point.  Again we save the value of the maximum normalized cross-correlation, and the value in days of the lag where it occurs, to compare with other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff_corr_amax = []\n",
    "runoff_lag_amax = []\n",
    "runoff_significance = []\n",
    "\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = nifl.Xcorr1D(xy, series_func=runoff_func, series_dates=d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, t_limits=(2009,2017), \n",
    "                              diff=1, normalize=True, pos_only=True)\n",
    "    ci_mod = F_runoff*np.asarray(ci)\n",
    "    runoff_corr_amax.append(corr[abs(corr).argmax()])\n",
    "    runoff_lag_amax.append(lags[abs(corr).argmax()])\n",
    "    runoff_significance.append(abs(corr[abs(corr).argmax()]) > ci_mod[abs(corr).argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminus position change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import width-averaged terminus position change processed by Leigh Stearns.  These data give terminus position in km from a baseline, so they do not need to be processed into a coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termini = pd.read_csv(termini_fpath, index_col=0, parse_dates=True, usecols=[0,1])\n",
    "trmn = termini.loc[termini.index.year >= 2006]\n",
    "tm = trmn.loc[trmn.index.year <2017].squeeze()\n",
    "\n",
    "## smooth a little to make more comparable with SMB and runoff\n",
    "td = tm.rolling('10D').mean() # approximately 3 measurements per window\n",
    "\n",
    "termini_d = [d.utctimetuple() for d in td.index]\n",
    "tm_d_interp = [ice.timeutils.datestr2tdec(d[0], d[1], d[2]) for d in termini_d]\n",
    "termini_func = interpolate.interp1d(tm_d_interp, td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the confidence interval correction factor for these data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_terminus = sm.tsa.stattools.acf(np.diff(td))[1]\n",
    "F_terminus = np.sqrt((1+(a_vel*b_terminus))/(1-(a_vel*b_terminus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, finding the maximum cross-correlation and the (positive) lag at which it occurs, as for other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminus_corr_amax = []\n",
    "terminus_lag_amax = []\n",
    "terminus_significance = []\n",
    "\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = nifl.Xcorr1D(xy, series_func=termini_func, series_dates=tm_d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, t_limits=(2009,2017), \n",
    "                              diff=1, normalize=True, pos_only=True)\n",
    "    ci_mod = F_terminus *np.asarray(ci)\n",
    "    terminus_corr_amax.append(corr[abs(corr).argmax()])\n",
    "    terminus_lag_amax.append(lags[abs(corr).argmax()])\n",
    "    terminus_significance.append(abs(corr[abs(corr).argmax()]) > ci_mod[abs(corr).argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial pattern of cross-correlation (Figure 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the patterns of cross-correlation for each variable, marking whether each value is significant.  Here we use a filled dot to indicate values significantly different from 0, and a cross to indicate values not significantly different from 0.  We will produce Figure 2 of the Ultee et al manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_colors = 'RdBu' # choose divergent colormap for xcorr\n",
    "corrnorm_min, corrnorm_max = -0.3, 0.3\n",
    "lag_colors = 'Greens'\n",
    "lagnorm_min, lagnorm_max = 0, 365\n",
    "\n",
    "sig_markers = ['o', 'x']\n",
    "\n",
    "ls = LightSource(azdeg=225, altdeg=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set matplotlib font size defaults\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## black-white hillshade topo underneath\n",
    "rgb2 = ls.shade(np.asarray(b_hel), cmap=plt.get_cmap('gray'), blend_mode='overlay',\n",
    "               dx=np.mean(np.diff(x_hel)), dy=np.mean(np.diff(y_hel)), vert_exag=5.)\n",
    "\n",
    "fig, ((ax1, ax2, ax3), (ax4,ax5,ax6)) = plt.subplots(nrows=2,ncols=3, figsize=(12, 8), \n",
    "                                                     # constrained_layout=True, \n",
    "                                                     sharex=True, sharey=True,\n",
    "                                                     gridspec_kw={'wspace':0.01})\n",
    "    \n",
    "ax1.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc1 = ax1.scatter(np.asarray(xys)[smb_significance,0], np.asarray(xys)[smb_significance,1], \n",
    "                  c=np.asarray(smb_corr_amax)[smb_significance], cmap=div_colors, marker=sig_markers[0], \n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "ax1.scatter(np.asarray(xys)[np.invert(smb_significance),0], np.asarray(xys)[np.invert(smb_significance),1], \n",
    "                  c=np.asarray(smb_corr_amax)[np.invert(smb_significance)], cmap=div_colors, marker=sig_markers[1], \n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max) #different marker for insig values\n",
    "ax1.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "        ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "       ylabel='Northing [km]', title='Catchment SMB')\n",
    "\n",
    "ax2.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc2 = ax2.scatter(np.asarray(xys)[runoff_significance,0], np.asarray(xys)[runoff_significance,1], \n",
    "                  c=np.asarray(runoff_corr_amax)[runoff_significance], cmap=div_colors, marker=sig_markers[0],\n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "ax2.scatter(np.asarray(xys)[np.invert(runoff_significance),0], np.asarray(xys)[np.invert(runoff_significance),1],\n",
    "            c=np.asarray(runoff_corr_amax)[np.invert(runoff_significance)], cmap=div_colors, marker=sig_markers[1],\n",
    "            vmin=corrnorm_min, vmax=corrnorm_max) # distinguish insig values\n",
    "ax2.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "       title='Catchment runoff')\n",
    "\n",
    "ax3.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc3 = ax3.scatter(np.asarray(xys)[terminus_significance,0], np.asarray(xys)[terminus_significance,1], \n",
    "                  c=np.asarray(terminus_corr_amax)[terminus_significance], cmap=div_colors, marker=sig_markers[0],\n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "ax3.scatter(np.asarray(xys)[np.invert(terminus_significance),0], np.asarray(xys)[np.invert(terminus_significance),1],\n",
    "            c=np.asarray(terminus_corr_amax)[np.invert(terminus_significance)], cmap=div_colors, marker=sig_markers[1],\n",
    "            vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "\n",
    "div3 = make_axes_locatable(ax3)\n",
    "cax3 = div3.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cb3 = fig.colorbar(sc3, cax=cax3)\n",
    "cb3.ax.set_ylabel('AMax. xcorr')\n",
    "ax3.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "       title='Terminus position', aspect=1.)\n",
    "\n",
    "## SECOND ROW\n",
    "ax4.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc4 = ax4.scatter(np.asarray(xys)[smb_significance,0], np.asarray(xys)[smb_significance,1], \n",
    "                  c=np.asarray(smb_lag_amax)[smb_significance], cmap=lag_colors, marker=sig_markers[0],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax4.scatter(np.asarray(xys)[np.invert(smb_significance),0], np.asarray(xys)[np.invert(smb_significance),1], \n",
    "            c=np.asarray(smb_lag_amax)[np.invert(smb_significance)], cmap=lag_colors, marker=sig_markers[1],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax4.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', ylabel='Northing [km]')\n",
    "\n",
    "ax5.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc5 = ax5.scatter(np.asarray(xys)[runoff_significance,0], np.asarray(xys)[runoff_significance,1], \n",
    "                  c=np.asarray(runoff_lag_amax)[runoff_significance], cmap=lag_colors, marker=sig_markers[0],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax5.scatter(np.asarray(xys)[np.invert(runoff_significance),0], np.asarray(xys)[np.invert(runoff_significance),1], \n",
    "            c=np.asarray(runoff_lag_amax)[np.invert(runoff_significance)], cmap=lag_colors, marker=sig_markers[1],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax5.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]')\n",
    "\n",
    "ax6.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc6 = ax6.scatter(np.asarray(xys)[terminus_significance,0], np.asarray(xys)[terminus_significance,1], \n",
    "                  c=np.asarray(terminus_lag_amax)[terminus_significance], cmap=lag_colors, marker=sig_markers[0],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax6.scatter(np.asarray(xys)[np.invert(terminus_significance),0], np.asarray(xys)[np.invert(terminus_significance),1], \n",
    "            c=np.asarray(terminus_lag_amax)[np.invert(terminus_significance)], cmap=lag_colors, marker=sig_markers[1],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "\n",
    "div6 = make_axes_locatable(ax6)\n",
    "cax6 = div6.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cb6 = fig.colorbar(sc6, cax=cax6)\n",
    "cb6.ax.set_ylabel('Lag [d] at peak xcorr')\n",
    "cb6.set_ticks([0, 60, 120, 180, 240, 300, 360])\n",
    "ax6.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', aspect=1.)\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "# plt.savefig('/Users/lizz/Desktop/20210204-helheim-xcorr_lag_composite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual chunks to compare changing seasonal cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break signals into annual subsets and compute the cross-correlation signal for each single year of data.  We will analyse patterns of these correlations at each point along flow, with color in the plot (Figure 3) indicating position.\n",
    "\n",
    "TODO: show map plot earlier so that lines can be matched with their associated point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_vals = {i: {'smb': [], 'runoff': [], 'terminus': []} \n",
    "               for i in range(len(xys))}\n",
    "date_chks = range(2009, 2018)\n",
    "\n",
    "for j in range(len(xys)):\n",
    "    point_to_plot = j\n",
    "    v_local = preds[point_to_plot]\n",
    "    a_vel = sm.tsa.stattools.acf(np.diff(v_local['full']))[1]\n",
    "\n",
    "    F_smb = np.sqrt((1+(a_vel*b_smb))/(1-(a_vel*b_smb))) ## compute CI correction factor using local velocity series\n",
    "    F_runoff = np.sqrt((1+(a_vel*b_runoff))/(1-(a_vel*b_runoff)))\n",
    "    F_terminus = np.sqrt((1+(a_vel*b_terminus))/(1-(a_vel*b_terminus)))\n",
    "    \n",
    "    \n",
    "    smb_annual_corrs = []\n",
    "    smb_annual_lags = []\n",
    "    smb_annual_ci = []\n",
    "    for i in range(len(date_chks)-1):\n",
    "        corr, lags, ci = nifl.Xcorr1D(xys[point_to_plot], series_func=smb_func, series_dates=smb_d_interp, \n",
    "                                  velocity_pred=preds[point_to_plot], t_grid=t_grid, t_limits=(date_chks[i], date_chks[i+1]),\n",
    "                                      diff=1, normalize=True, pos_only=True)\n",
    "        ci_mod = F_smb*np.asarray(ci) # correct the significance interval\n",
    "        smb_annual_corrs.append(corr)\n",
    "        smb_annual_lags.append(lags)\n",
    "        smb_annual_ci.append(ci_mod)\n",
    "    annual_vals[j]['smb'] = smb_annual_corrs\n",
    "\n",
    "    \n",
    "    rf_annual_corrs = []\n",
    "    rf_annual_lags = []\n",
    "    rf_annual_ci = []\n",
    "    for i in range(len(date_chks)-1):\n",
    "        corr, lags, ci = nifl.Xcorr1D(xys[point_to_plot], series_func=runoff_func, series_dates=d_interp, \n",
    "                                  velocity_pred=preds[point_to_plot], t_grid=t_grid, t_limits=(date_chks[i], date_chks[i+1]),\n",
    "                                      diff=1, normalize=True, pos_only=True)\n",
    "        ci_mod = F_runoff*np.asarray(ci) # correct the significance interval\n",
    "        rf_annual_corrs.append(corr)\n",
    "        rf_annual_lags.append(lags)\n",
    "        rf_annual_ci.append(ci_mod)\n",
    "    annual_vals[j]['runoff'] = rf_annual_corrs\n",
    "    \n",
    "    \n",
    "    tm_annual_corrs = []\n",
    "    tm_annual_lags = []\n",
    "    tm_annual_ci = []\n",
    "    for i in range(len(date_chks)-1):\n",
    "        corr, lags, ci = nifl.Xcorr1D(xys[point_to_plot], series_func=termini_func, series_dates=tm_d_interp, \n",
    "                                  velocity_pred=preds[point_to_plot], t_grid=t_grid, t_limits=(date_chks[i], date_chks[i+1]),\n",
    "                                      diff=1, normalize=True, pos_only=True)\n",
    "        ci_mod = F_terminus *np.asarray(ci) # correct the significance interval\n",
    "        tm_annual_corrs.append(corr)\n",
    "        tm_annual_lags.append(lags)\n",
    "        tm_annual_ci.append(ci_mod)\n",
    "    annual_vals[j]['terminus'] = tm_annual_corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the cross-correlations of annual subsetted signals in a stack.  Each panel of the below will represent cross-correlation between velocity and a single variable (columns) for a single year (rows).  The colored lines in each panel correspond to points along the flowline defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clrs = plt.get_cmap('plasma')(np.array(range(len(xys)))/len(xys))\n",
    "corr_line_width=2.0\n",
    "\n",
    "fig, axs = plt.subplots(len(rf_annual_corrs), ncols=3, figsize=(8, 13), sharex=True, sharey=True)\n",
    "for i in range(len(smb_annual_ci)):\n",
    "    ax = axs[i][0]\n",
    "    ax.axvline(x=0, color='k', alpha=0.5)\n",
    "    ax.axhline(y=0, color='k', alpha=0.5)\n",
    "    ax.plot(smb_annual_lags[i], smb_annual_ci[i], ls=':', color='k')\n",
    "    ax.plot(smb_annual_lags[i], -1*np.array(smb_annual_ci[i]), ls=':', color='k')\n",
    "    for n in range(len(xys)):\n",
    "        corr_color = clrs[n]\n",
    "        ax.plot(smb_annual_lags[i], annual_vals[n]['smb'][i], color=corr_color, \n",
    "                lw=corr_line_width, alpha=0.3)\n",
    "#         ax.fill_between(smb_annual_lags[i], y1=annual_vals[n]['smb'][i], y2=0, \n",
    "#                         where=abs(annual_vals[n]['smb'][i])>smb_annual_ci[i], color=corr_color, alpha=0.2)\n",
    "        ax.fill_between(smb_annual_lags[i], y1=-1, y2=1, \n",
    "                        where=abs(annual_vals[n]['smb'][i])>smb_annual_ci[i], color=corr_color, alpha=0.1)\n",
    "    if i==0:\n",
    "        ax.set(title='SMB', ylabel='xcorr')\n",
    "    elif i==len(axs)-1:\n",
    "        ax.set(xlabel='Lag [d]', ylabel='xcorr')\n",
    "    else:\n",
    "        ax.set(ylabel='xcorr')\n",
    "for j in range(len(rf_annual_ci)):\n",
    "    ax = axs[j][1]\n",
    "    ax.axvline(x=0, color='k', alpha=0.5)\n",
    "    ax.axhline(y=0, color='k', alpha=0.5)\n",
    "    ax.plot(rf_annual_lags[j], rf_annual_ci[j], ls=':', color='k')\n",
    "    ax.plot(rf_annual_lags[j], -1*np.array(rf_annual_ci[j]), ls=':', color='k')\n",
    "    for n in range(len(xys)):\n",
    "        corr_color = clrs[n]\n",
    "        ax.plot(rf_annual_lags[j], annual_vals[n]['runoff'][j], color=corr_color, \n",
    "                lw=corr_line_width, alpha=0.3)\n",
    "#         ax.fill_between(rf_annual_lags[j], y1=annual_vals[n]['runoff'][j], y2=0, \n",
    "#                         where=abs(annual_vals[n]['runoff'][j])>rf_annual_ci[j], color=corr_color, alpha=0.1)\n",
    "        ax.fill_between(rf_annual_lags[j], y1=-1, y2=1, \n",
    "                        where=abs(annual_vals[n]['runoff'][j])>rf_annual_ci[j], color=corr_color, alpha=0.1)\n",
    "    if j==0:\n",
    "        ax.set(title='Runoff')\n",
    "    elif j==len(axs)-1:\n",
    "        ax.set(xlabel='Lag [d]')\n",
    "    else:\n",
    "        continue\n",
    "for k in range(len(tm_annual_ci)):\n",
    "    ax = axs[k][2]\n",
    "    ax.axvline(x=0, color='k', alpha=0.5)\n",
    "    ax.axhline(y=0, color='k', alpha=0.5)\n",
    "    ax.plot(tm_annual_lags[k], tm_annual_ci[k], ls=':', color='k')\n",
    "    ax.plot(tm_annual_lags[k], -1*np.array(tm_annual_ci[k]), ls=':', color='k')\n",
    "    for n in range(len(xys)):\n",
    "        corr_color = clrs[n]\n",
    "        ax.plot(tm_annual_lags[k], annual_vals[n]['terminus'][k], color=corr_color, \n",
    "                lw=corr_line_width, alpha=0.3)\n",
    "#         ax.fill_between(tm_annual_lags[k], y1=annual_vals[n]['terminus'][k], y2=0, \n",
    "#                         where=abs(annual_vals[n]['terminus'][k])>tm_annual_ci[k], color=corr_color, alpha=0.2)\n",
    "        ax.fill_between(tm_annual_lags[k], y1=-1, y2=1, \n",
    "                        where=abs(annual_vals[n]['terminus'][k])>tm_annual_ci[k], color=corr_color, alpha=0.1)\n",
    "    ax.text(250, 0.55, str(date_chks[k]), ha='center', size=10, weight=500, color='k')\n",
    "    if k==0:\n",
    "        ax.set(title='Terminus pos.')\n",
    "    elif k==len(axs)-1:\n",
    "        ax.set(xlabel='Lag [d]')\n",
    "    else:\n",
    "        continue\n",
    "for ax in axs.ravel():\n",
    "    ax.set(ylim=(-1,1), \n",
    "            xlim=(0,360),\n",
    "            xticks=(0, 90, 180, 270, 360)\n",
    "#            xticks=(-360, -180, 0, 180, 360)\n",
    "           )\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(30))\n",
    "plt.tight_layout()\n",
    "# plt.savefig('/Users/lizz/Desktop/{}-annual_chunk-allvars.png'.format(datetime.date.today().strftime('%Y%m%d')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate multi-annual components of signal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding analyses studied signals whose predominant variability was at seasonal scale.  Now, we will look at cross-correlations between the components of the signals that vary over longer (multi-annual) periods.  This can help us understand whether the dominant forcings of velocity variability are consistent or different across temporal scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a new version of our 1-D cross-correlation function.  Instead of using the `full` version of the continuous velocity series generated by `iceutils`, this one selects the `secular` and `transient` variation components (i.e. no seasonal variation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xcorr1D_lt(pt, series_func, series_dates, velocity_pred, t_grid, t_limits, \n",
    "               diff=1, normalize=True, pos_only=False):\n",
    "    \"\"\"\n",
    "    Compute cross-correlation on coincident series of a 1D time series\n",
    "    (e.g. catchment-integrated runoff or SMB) versus velocity at a point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pt : tuple\n",
    "        Position (x,y) at which to pull velocity series.\n",
    "    series_func : interpolate.interp1d\n",
    "        1D-interpolated function with values of data series over time.\n",
    "    series_dates : list\n",
    "        Decimal dates of data points\n",
    "    velocity_series : dict\n",
    "        Output of iceutils prediction.\n",
    "    t_grid : ndarray\n",
    "        Evenly spaced decimal times at which spline-fit velocity is sampled\n",
    "    t_limits : tuple\n",
    "        Start and end dates (decimal) of the time period to study\n",
    "    diff : int, optional\n",
    "        Number of discrete differences to apply to data. Default is 1.\n",
    "        Setting diff=0 will process the input data as-is.\n",
    "    normalize : bool, optional\n",
    "        Whether to normalize for a cross-correlation in [-1,1]. Default is True.\n",
    "        This makes the output inter-comparable with normalized output for other\n",
    "        variables.  If set to False, the signal amplitude will be larger but\n",
    "        the correlation values may exceed 1.\n",
    "    pos_only : bool, optional\n",
    "    \tWhether to analyse only xcorrs with positive lag values.  Default is False.\n",
    "    \tThis allows a bidirectional causal relationship.  For a causal relationship \n",
    "    \thypothesised to be single-directional, choose True to display only positive\n",
    "    \tlag values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corr : array\n",
    "        Cross-correlation coefficients between SMB, velocity\n",
    "    lags : array\n",
    "        Time lag for each correlation value\n",
    "    ci : array\n",
    "        Confidence intervals for evaluation\n",
    "\n",
    "    \"\"\"\n",
    "    t_min = max(min(series_dates), t_limits[0])\n",
    "    t_max = min(max(series_dates), t_limits[1])\n",
    "    coincident_dates = np.asarray([t for t in t_grid if (t>=t_min and t<t_max)])\n",
    "    coincident_series = series_func(coincident_dates) # sample at same dates as velocity series\n",
    "    \n",
    "    vel_longterm = velocity_pred['secular'] + velocity_pred['transient']\n",
    "    series_diff = np.diff(coincident_series, n=diff)\n",
    "    vel_series_0 = vel_longterm[np.where(t_grid>=t_min)]\n",
    "    vel_series = vel_series_0[np.where(t_grid[np.where(t_grid>=t_min)]<t_max)] # trim dates to match t_limits\n",
    "    vel_diff = np.diff(vel_series, n=diff)\n",
    "    if normalize:\n",
    "        series_diff = (series_diff-np.mean(series_diff)) / (np.std(series_diff)*len(series_diff))\n",
    "        vel_diff = (vel_diff-np.mean(vel_diff)) / (np.std(vel_diff))\n",
    "    corr = np.correlate(series_diff, vel_diff, mode='full')\n",
    "    lags = range(int(-0.5*len(corr)), int(0.5*len(corr)+1))\n",
    "    ci = [2/np.sqrt(len(coincident_series)-abs(k)) for k in lags]\n",
    "\n",
    "    ## convert lags to physical units\n",
    "    lags = np.mean(np.diff(t_grid))*365.26*np.asarray(lags)\n",
    "    \n",
    "    if pos_only:\n",
    "    \tcorr = corr[np.argwhere(lags>=0)].squeeze()\n",
    "    \tci = np.asarray(ci)[np.argwhere(lags>=0)].squeeze()\n",
    "    \tlags = lags[np.argwhere(lags>=0)].squeeze()\n",
    "    \n",
    "    return corr, lags, ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the velocity signal will already be using only the long-term-varying components.  Our time series of system variables have not been generated with `iceutils`, so instead of using component selection on them, we will apply a simple boxcar filter to remove short-term variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_grid_trimmed = t_grid[np.argwhere(t_grid<2017)] ## valid range for interpolated funcs\n",
    "window = int(1./np.mean(np.diff(t_grid_trimmed.squeeze()))) # set window size to the number of time steps in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Low-frequency SMB variability\n",
    "smb_evensampled = 1E-9*np.array(smb_func(t_grid_trimmed).squeeze())\n",
    "smb_filtered = ndimage.uniform_filter1d(smb_evensampled, size=window)\n",
    "smb_lowfreq = interpolate.UnivariateSpline(t_grid_trimmed, smb_filtered, s=0)\n",
    "\n",
    "a_vel_lt = sm.tsa.stattools.acf(preds[0]['secular']+preds[0]['transient'])[1] ## CI correction factor for long-term-varying signals\n",
    "b_smb_lt = sm.tsa.stattools.acf(smb_lowfreq(t_grid))[1]\n",
    "F_smb_lt = np.sqrt((1+(a_vel_lt*b_smb_lt))/(1-(a_vel_lt*b_smb_lt)))\n",
    "\n",
    "smb_lt_corr_amax = []\n",
    "smb_lt_lag_amax = []\n",
    "smb_lt_significance = []\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = Xcorr1D_lt(xy, series_func=smb_lowfreq, series_dates=smb_d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, t_limits=(2009,2017), \n",
    "                              diff=0, normalize=True, pos_only=True)\n",
    "    ci_mod = F_smb_lt * np.asarray(ci)\n",
    "    smb_lt_corr_amax.append(corr[abs(corr).argmax()])\n",
    "    smb_lt_lag_amax.append(lags[abs(corr).argmax()])\n",
    "    smb_lt_significance.append(abs(corr[abs(corr).argmax()]) > ci_mod[abs(corr).argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Low-frequency runoff variability\n",
    "rnf_evensampled = runoff_func(t_grid_trimmed).squeeze()\n",
    "rnf_filtered = ndimage.uniform_filter1d(rnf_evensampled, size=window)\n",
    "rf_lowfreq = interpolate.UnivariateSpline(t_grid_trimmed, rnf_filtered, s=0)\n",
    "\n",
    "b_runoff_lt = sm.tsa.stattools.acf(rf_lowfreq(t_grid))[1]\n",
    "F_runoff_lt = np.sqrt((1+(a_vel_lt*b_runoff_lt))/(1-(a_vel_lt*b_runoff_lt)))\n",
    "\n",
    "rf_lt_corr_amax = []\n",
    "rf_lt_lag_amax = []\n",
    "rf_lt_significance = []\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = Xcorr1D_lt(xy, series_func=rf_lowfreq, series_dates=d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, t_limits=(2009,2017), \n",
    "                              diff=0, normalize=True, pos_only=True)\n",
    "    ci_mod = F_runoff_lt * np.asarray(ci)\n",
    "    rf_lt_corr_amax.append(corr[abs(corr).argmax()])\n",
    "    rf_lt_lag_amax.append(lags[abs(corr).argmax()])\n",
    "    rf_lt_significance.append(abs(corr[abs(corr).argmax()]) > ci_mod[abs(corr).argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Low-frequency terminus variability\n",
    "tm_evensampled = termini_func(t_grid_trimmed).squeeze()\n",
    "tm_filtered = ndimage.uniform_filter1d(tm_evensampled, size=window)\n",
    "tf_lowfreq = interpolate.UnivariateSpline(t_grid_trimmed, tm_filtered, s=0)\n",
    "\n",
    "b_terminus_lt = sm.tsa.stattools.acf(tf_lowfreq(t_grid))[1]\n",
    "F_terminus_lt = np.sqrt((1+(a_vel_lt*b_terminus_lt))/(1-(a_vel_lt*b_terminus_lt)))\n",
    "\n",
    "term_lt_corr_amax = []\n",
    "term_lt_lag_amax = []\n",
    "terminus_lt_significance = []\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = Xcorr1D_lt(xy, series_func=tf_lowfreq, series_dates=tm_d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, t_limits=(2009,2017), \n",
    "                              diff=0, normalize=True, pos_only=True)\n",
    "    ci_mod = F_terminus_lt *np.asarray(ci)\n",
    "    term_lt_corr_amax.append(corr[abs(corr).argmax()])\n",
    "    term_lt_lag_amax.append(lags[abs(corr).argmax()])\n",
    "    terminus_lt_significance.append(abs(corr[abs(corr).argmax()]) > ci_mod[abs(corr).argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate another six-panel composite showing the maximum cross-correlation and the lag at which it occurs, similar to Figure 2 in the earlier section, but this time for the long-term-varying components as computed above.  The resulting plot is Figure S6 of the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4,ax5,ax6)) = plt.subplots(nrows=2,ncols=3, figsize=(12, 8), \n",
    "                                                      # constrained_layout=True, \n",
    "                                                      sharex=True, sharey=True,\n",
    "                                                      gridspec_kw={'wspace':0.01})\n",
    "    \n",
    "ax1.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc1 = ax1.scatter(np.asarray(xys)[smb_lt_significance,0], np.asarray(xys)[smb_lt_significance,1], \n",
    "                  c=np.asarray(smb_lt_corr_amax)[smb_lt_significance], cmap=div_colors, marker=sig_markers[0],\n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "ax1.scatter(np.asarray(xys)[np.invert(smb_lt_significance),0], np.asarray(xys)[np.invert(smb_lt_significance),1], \n",
    "                  c=np.asarray(smb_lt_corr_amax)[np.invert(smb_lt_significance)], cmap=div_colors, marker=sig_markers[1],\n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "# ## set up correctly scaled colorbar\n",
    "# div1 = make_axes_locatable(ax1)\n",
    "# cax1 = div1.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "# plt.colorbar(sc1, cax=cax1)\n",
    "# cb1.ax.set_title('AMax. xcorr')\n",
    "ax1.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "        ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "        xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "        ylabel='Northing [km]', title='Catchment SMB')\n",
    "\n",
    "ax2.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc2 = ax2.scatter(np.asarray(xys)[rf_lt_significance,0], np.asarray(xys)[rf_lt_significance,1], \n",
    "                  c=np.asarray(rf_lt_corr_amax)[rf_lt_significance], cmap=div_colors, marker=sig_markers[0],\n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "ax2.scatter(np.asarray(xys)[np.invert(rf_lt_significance),0], np.asarray(xys)[np.invert(rf_lt_significance),1], \n",
    "                  c=np.asarray(rf_lt_corr_amax)[np.invert(rf_lt_significance)], cmap=div_colors, marker=sig_markers[1],\n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "# ## set up correctly scaled colorbar\n",
    "# div2 = make_axes_locatable(ax2)\n",
    "# cax2 = div2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "# fig.colorbar(sc2, cax=cax2)\n",
    "# cb2.ax.set_title('AMax. xcorr')\n",
    "ax2.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "        xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "        title='Catchment runoff')\n",
    "\n",
    "ax3.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc3 = ax3.scatter(np.asarray(xys)[terminus_lt_significance,0], np.asarray(xys)[terminus_lt_significance,1], \n",
    "                  c=np.asarray(term_lt_corr_amax)[terminus_lt_significance], cmap=div_colors, marker=sig_markers[0],\n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "ax3.scatter(np.asarray(xys)[np.invert(terminus_lt_significance),0], np.asarray(xys)[np.invert(terminus_lt_significance),1], \n",
    "                  c=np.asarray(term_lt_corr_amax)[np.invert(terminus_lt_significance)], cmap=div_colors, marker=sig_markers[1],\n",
    "                  vmin=corrnorm_min, vmax=corrnorm_max)\n",
    "## set up correctly scaled colorbar - one for all xcorr plots\n",
    "div3 = make_axes_locatable(ax3)\n",
    "cax3 = div3.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cb3 = fig.colorbar(sc3, cax=cax3, extend='both')\n",
    "cb3.ax.set_ylabel('AMax. xcorr')\n",
    "ax3.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "        xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "        title='Terminus position', aspect=1.)\n",
    "\n",
    "## SECOND ROW\n",
    "ax4.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc4 = ax4.scatter(np.asarray(xys)[smb_lt_significance,0], np.asarray(xys)[smb_lt_significance,1], \n",
    "                  c=np.asarray(smb_lt_lag_amax)[smb_lt_significance], cmap=lag_colors, marker=sig_markers[0],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax4.scatter(np.asarray(xys)[np.invert(smb_lt_significance),0], np.asarray(xys)[np.invert(smb_lt_significance),1], \n",
    "                  c=np.asarray(smb_lt_lag_amax)[np.invert(smb_lt_significance)], cmap=lag_colors, marker=sig_markers[1],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "# ## set up correctly scaled colorbar\n",
    "# div4 = make_axes_locatable(ax4)\n",
    "# cax4 = div4.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "# plt.colorbar(sc4, cax=cax4)\n",
    "# cb1.ax.set_title('Lag [d] at peak xcorr')\n",
    "ax4.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "        xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', ylabel='Northing [km]')\n",
    "\n",
    "ax5.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc5 = ax5.scatter(np.asarray(xys)[rf_lt_significance,0], np.asarray(xys)[rf_lt_significance,1], \n",
    "                  c=np.asarray(rf_lt_lag_amax)[rf_lt_significance], cmap=lag_colors, marker=sig_markers[0],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax5.scatter(np.asarray(xys)[np.invert(rf_lt_significance),0], np.asarray(xys)[np.invert(rf_lt_significance),1], \n",
    "                  c=np.asarray(rf_lt_lag_amax)[np.invert(rf_lt_significance)], cmap=lag_colors, marker=sig_markers[1],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "\n",
    "# ## set up correctly scaled colorbar\n",
    "# div5 = make_axes_locatable(ax5)\n",
    "# cax5 = div5.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "# fig.colorbar(sc5, cax=cax5)\n",
    "# cb2.ax.set_title('Lag [d] at peak xcorr')\n",
    "ax5.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "        xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]')\n",
    "\n",
    "ax6.imshow(rgb2, origin='lower', extent=(x_hel[0], x_hel[-1], y_hel[0], y_hel[-1]))\n",
    "sc6 = ax6.scatter(np.asarray(xys)[terminus_lt_significance,0], np.asarray(xys)[terminus_lt_significance,1], \n",
    "                  c=np.asarray(term_lt_lag_amax)[terminus_lt_significance], cmap=lag_colors, marker=sig_markers[0],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "ax6.scatter(np.asarray(xys)[np.invert(terminus_lt_significance),0], np.asarray(xys)[np.invert(terminus_lt_significance),1], \n",
    "                  c=np.asarray(term_lt_lag_amax)[np.invert(terminus_lt_significance)], cmap=lag_colors, marker=sig_markers[1],\n",
    "                  vmin=lagnorm_min, vmax=lagnorm_max)\n",
    "\n",
    "## set up correctly scaled colorbar\n",
    "div6 = make_axes_locatable(ax6)\n",
    "cax6 = div6.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cb6 = fig.colorbar(sc6, cax=cax6, extend='min')\n",
    "cb6.ax.set_ylabel('Lag [d] at peak xcorr')\n",
    "ax6.set(xlim=(278000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "        xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', aspect=1.)\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig('/Users/lizz/Desktop/20210204-helheim-longterm_xcorr_lag_composite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the influence of subglacial topography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plots, we have seen an interesting pattern emerge: there appears to be a clear separation of \"upstream\" and \"downstream\" type behaviour.  There is a topographic feature evidence from the maps that could be creating this separation.  Let's now look at an along-flow profile of the subglacial topography to get a better idea of its shape.  We will also compare the seasonal and long-term-varying correlations of each system variable with velocity, and we'll look at the pattern of velocity along flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract bed topo along flowline\n",
    "xyvals = np.array([(xh[i], yh[i]) for i in range(len(xh))]) ## a dense sampling, more than 9000 points\n",
    "bed_vals = [float(B_helheim(xh[i], yh[i])) for i in range(len(xh))]\n",
    "surface_vals = [float(S_helheim(xh[i], yh[i])) for i in range(len(xh))]\n",
    "xvals = (0.001*np.array(nifl.ArcArray(xyvals)))\n",
    "\n",
    "## Create arclength array to plot points along flowline\n",
    "s = 0.001*np.array(nifl.ArcArray(np.array(xys)))+2. # account for the 2km that was removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute mean ice speed for each point\n",
    "mean_speed = [np.mean(pred['full']) for pred in preds]\n",
    "scaled_speed = [m/np.mean(mean_speed) for m in mean_speed]\n",
    "scaled_ticks = (np.array([4.0, 6, 8.0])/np.mean(mean_speed)) - np.mean(scaled_speed) # for display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a stacked plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Introduce vertical offsets and scaling to make a clean stack with only one set of axes\n",
    "smb_offset=9000 # how many 'meters' above topography baseline to plot this line\n",
    "runoff_offset=7000 \n",
    "terminus_offset=5000\n",
    "velocity_offset=2500 \n",
    "scaling=1000  # vertical multiplicative factor to display xcorr on same axes as topo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are going to take a shortcut in the plotting here: We know from the section above that with our usual settings, _none_ of the cross-correlations with long-term-varying SMB or runoff are significantly different from 0.  Thus, instead of testing each array for significance again, we are going to directly plot crosses for \"not significant\".  If you change settings above, be aware that you will need to account for any changes in significance below.\n",
    "\n",
    "TODO: add automated handling as in the 6-panel composites above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_colors = cm.get_cmap('tab20b')\n",
    "\n",
    "fig, ax = plt.subplots(1, constrained_layout=True)\n",
    "\n",
    "# ax.axvline(10, color='k', lw=1.0, ls='--')\n",
    "ax.axvline(14, color='k', lw=0.5, ls='--')\n",
    "# ax.plot(10*np.ones_like(s), np.linspace(-1300, smb_offset, len(s)), color='k', lw=1.0, ls='--')\n",
    "ax.plot(s, smb_offset*np.ones_like(s), color='k', lw=0.5, ls=':')\n",
    "ax.plot(s, runoff_offset*np.ones_like(s), color='k', lw=0.5, ls=':')\n",
    "ax.plot(s, terminus_offset*np.ones_like(s), color='k', lw=0.5, ls=':')\n",
    "\n",
    "## SMB stems\n",
    "markerline, stemlines, baseline = ax.stem(s, smb_offset+ scaling*np.array(smb_lt_corr_amax),\n",
    "                                          bottom=smb_offset, basefmt='k:',label='SMB long-term',\n",
    "                                          markerfmt='x')\n",
    "plt.setp(stemlines, 'color', qual_colors(1))\n",
    "plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "plt.setp(markerline, 'color', qual_colors(1))\n",
    "\n",
    "markerline, stemlines, baseline = ax.stem(s, smb_offset+ scaling*np.array(smb_corr_amax),\n",
    "                                          bottom=smb_offset, basefmt='k:',label='SMB')\n",
    "plt.setp(stemlines, 'color', qual_colors(0))\n",
    "plt.setp(markerline, 'color', qual_colors(0))\n",
    "\n",
    "\n",
    "## Runoff stems\n",
    "markerline, stemlines, baseline = ax.stem(s, runoff_offset+ scaling*np.array(rf_lt_corr_amax),\n",
    "                                          bottom=runoff_offset, basefmt='k:',label='Runoff long-term',\n",
    "                                          markerfmt='x')\n",
    "plt.setp(stemlines, 'color', qual_colors(3))\n",
    "plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "plt.setp(markerline, 'color', qual_colors(3))\n",
    "\n",
    "markerline, stemlines, baseline = ax.stem(s, runoff_offset+ scaling*np.array(runoff_corr_amax),\n",
    "                                          bottom=runoff_offset, basefmt='k:',label='Runoff')\n",
    "plt.setp(stemlines, 'color', qual_colors(2))\n",
    "plt.setp(markerline, 'color', qual_colors(2))\n",
    "\n",
    "\n",
    "## Terminus stems\n",
    "markerline, stemlines, baseline = ax.stem(s, terminus_offset+ scaling*np.array(term_lt_corr_amax),\n",
    "                                          bottom=terminus_offset, basefmt='k:',label='Terminus long-term',\n",
    "                                          markerfmt='d')\n",
    "plt.setp(stemlines, 'color', qual_colors(5))\n",
    "plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "plt.setp(markerline, 'color', qual_colors(5))\n",
    "\n",
    "markerline, stemlines, baseline = ax.stem(s, terminus_offset+ scaling*np.array(terminus_corr_amax),\n",
    "                                          bottom=terminus_offset, basefmt='k:',label='Terminus')\n",
    "plt.setp(stemlines, 'color', qual_colors(4))\n",
    "plt.setp(markerline, 'color', qual_colors(4))\n",
    "\n",
    "## add velocity\n",
    "ax.plot(s, velocity_offset+scaling*np.array(scaled_speed - np.mean(scaled_speed)), color=qual_colors(17), label='Mean speed', lw=2.5)\n",
    "\n",
    "\n",
    "ax.plot(xvals, bed_vals, color='saddlebrown', lw=2.0)\n",
    "ax.plot(xvals, surface_vals, color='darkgrey', lw=2.0)\n",
    "plt.fill_between(xvals, surface_vals, bed_vals, color='darkgrey', alpha=0.5)\n",
    "plt.fill_between(xvals, bed_vals, y2=-1300, color='saddlebrown', alpha=0.5, hatch='/')\n",
    "ax.set(xlim=(25, 0), ylim=(-1300, 10000), aspect=0.002, \n",
    "        xlabel='Upstream distance [km]',\n",
    "        yticks=(-1000, 0, 1000, \n",
    "                velocity_offset+scaling*scaled_ticks[1],\n",
    "                terminus_offset,  \n",
    "                runoff_offset, \n",
    "                smb_offset),\n",
    "        yticklabels=('-1000', '0', '1000 m a.s.l.', \n",
    "                     'Surface speed',\n",
    "                     'Terminus xcorr',\n",
    "                     'Runoff xcorr',\n",
    "                     'SMB xcorr'))\n",
    "minor_locator = ticker.FixedLocator([velocity_offset+scaling*scaled_ticks[0], velocity_offset+scaling*scaled_ticks[2],\n",
    "                              terminus_offset-0.5*scaling, terminus_offset+0.5*scaling, \n",
    "                              runoff_offset-0.5*scaling, runoff_offset+0.5*scaling,\n",
    "                              smb_offset-0.5*scaling, smb_offset+0.5*scaling])\n",
    "minor_formatter = ticker.FixedFormatter([4, '8 km/a',\n",
    "                                         -0.5, 0.5, \n",
    "                                        -0.5, 0.5,\n",
    "                                        -0.5, 0.5])\n",
    "ax.yaxis.set_minor_locator(minor_locator)\n",
    "ax.yaxis.set_minor_formatter(minor_formatter)\n",
    "ax.tick_params(axis='both', which='major', length=7)\n",
    "ax.tick_params(axis='y', which='minor', length=4)\n",
    "ax.get_yticklabels()[3].set_color(qual_colors(17)) # match the ticks to the plotted color\n",
    "ax.get_yticklabels()[4].set_color(qual_colors(4)) \n",
    "ax.get_yticklabels()[5].set_color(qual_colors(2))\n",
    "ax.get_yticklabels()[6].set_color(qual_colors(0))\n",
    "\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.savefig('/Users/Lizz/Desktop/{}-helheim-along_flow_stack.png'.format(datetime.date.today()), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "mtypes = ['o', 'd', 'x']\n",
    "labels = ['Short term (sig. at 95%)', 'Long term (sig. at 95%)', 'Not significant']\n",
    "lines = [Line2D([0], [0], color='k', linewidth=0, marker=m) for m in mtypes]\n",
    "plt.legend(lines, labels, loc='upper right', bbox_to_anchor=(0.5, 0.5))\n",
    "for sp in ('top', 'bottom', 'left', 'right'):\n",
    "    ax.spines[sp].set_visible(False)\n",
    "    ax.tick_params(which='both', length=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the supplementary material of our manuscript, we present additional figures including autocorrelation functions and correlograms.  Stay tuned for those plots to be added to this walkthrough.  In the meantime, you can find scripts to produce those figures in the Manuscript-figures folder of [this GitHub repository](http://github.com/ehultee/helheim-fiesta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a stack, this time with lags included, as requested by reviewer 2\n",
    "\n",
    "## Introduce vertical offsets and scaling to make a clean stack with only one set of axes\n",
    "smb_offset=15000 # how many 'meters' above topography baseline to plot this line\n",
    "smb_lag_offset=13000\n",
    "runoff_offset=11000 \n",
    "runoff_lag_offset=9000\n",
    "terminus_offset=7000\n",
    "terminus_lag_offset=5000\n",
    "velocity_offset=2500 \n",
    "scaling=1000  # vertical multiplicative factor to display xcorr on same axes as topo\n",
    "lag_scaling=1 # multiplicative factor to display lag on same axes\n",
    "\n",
    "capped_smb_lags = np.where(np.asarray(smb_lag_amax)>365, 365, smb_lag_amax)\n",
    "capped_runoff_lags = np.where(np.asarray(runoff_lag_amax)>365, 365, runoff_lag_amax)\n",
    "\n",
    "qual_colors = cm.get_cmap('tab20b')\n",
    "\n",
    "fig, ax = plt.subplots(1, constrained_layout=True, figsize=(5,10))\n",
    "\n",
    "# ax.axvline(10, color='k', lw=1.0, ls='--')\n",
    "ax.axvline(14, color='k', lw=0.5, ls='--')\n",
    "# ax.plot(10*np.ones_like(s), np.linspace(-1300, smb_offset, len(s)), color='k', lw=1.0, ls='--')\n",
    "ax.plot(s, smb_offset*np.ones_like(s), color='k', lw=0.5, ls=':')\n",
    "ax.plot(s, runoff_offset*np.ones_like(s), color='k', lw=0.5, ls=':')\n",
    "ax.plot(s, terminus_offset*np.ones_like(s), color='k', lw=0.5, ls=':')\n",
    "\n",
    "## SMB stems\n",
    "# markerline, stemlines, baseline = ax.stem(s, smb_offset+ scaling*np.array(smb_lt_corr_amax),\n",
    "#                                           bottom=smb_offset, basefmt='k:',label='SMB long-term',\n",
    "#                                           markerfmt='x')\n",
    "# plt.setp(stemlines, 'color', qual_colors(1))\n",
    "# plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "# plt.setp(markerline, 'color', qual_colors(1))\n",
    "\n",
    "markerline, stemlines, baseline = ax.stem(s, smb_offset+ scaling*np.array(smb_corr_amax),\n",
    "                                          bottom=smb_offset, basefmt='k:',label='SMB')\n",
    "plt.setp(stemlines, 'color', qual_colors(0))\n",
    "plt.setp(markerline, 'color', qual_colors(0))\n",
    "\n",
    "## SMB lags\n",
    "markerline, stemlines, baseline = ax.stem(s, smb_lag_offset+ lag_scaling*np.array(capped_smb_lags),\n",
    "                                          bottom=smb_lag_offset, basefmt='k:',label='SMB lags',\n",
    "                                          markerfmt='1')\n",
    "plt.setp(stemlines, 'color', qual_colors(0))\n",
    "plt.setp(markerline, 'color', qual_colors(0))\n",
    "\n",
    "## Runoff stems\n",
    "# markerline, stemlines, baseline = ax.stem(s, runoff_offset+ scaling*np.array(rf_lt_corr_amax),\n",
    "#                                           bottom=runoff_offset, basefmt='k:',label='Runoff long-term',\n",
    "#                                           markerfmt='x')\n",
    "# plt.setp(stemlines, 'color', qual_colors(3))\n",
    "# plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "# plt.setp(markerline, 'color', qual_colors(3))\n",
    "\n",
    "markerline, stemlines, baseline = ax.stem(s, runoff_offset+ scaling*np.array(runoff_corr_amax),\n",
    "                                          bottom=runoff_offset, basefmt='k:',label='Runoff')\n",
    "plt.setp(stemlines, 'color', qual_colors(2))\n",
    "plt.setp(markerline, 'color', qual_colors(2))\n",
    "\n",
    "\n",
    "## Runoff lags\n",
    "markerline, stemlines, baseline = ax.stem(s, runoff_lag_offset+ lag_scaling*np.array(capped_runoff_lags),\n",
    "                                          bottom=runoff_lag_offset, basefmt='k:',label='Runoff lags',\n",
    "                                          markerfmt='1')\n",
    "plt.setp(stemlines, 'color', qual_colors(2))\n",
    "plt.setp(markerline, 'color', qual_colors(2))\n",
    "\n",
    "## Terminus stems\n",
    "# markerline, stemlines, baseline = ax.stem(s, terminus_offset+ scaling*np.array(term_lt_corr_amax),\n",
    "#                                           bottom=terminus_offset, basefmt='k:',label='Terminus long-term',\n",
    "#                                           markerfmt='d')\n",
    "# plt.setp(stemlines, 'color', qual_colors(5))\n",
    "# plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "# plt.setp(markerline, 'color', qual_colors(5))\n",
    "\n",
    "markerline, stemlines, baseline = ax.stem(s, terminus_offset+ scaling*np.array(terminus_corr_amax),\n",
    "                                          bottom=terminus_offset, basefmt='k:',label='Terminus')\n",
    "plt.setp(stemlines, 'color', qual_colors(4))\n",
    "plt.setp(markerline, 'color', qual_colors(4))\n",
    "\n",
    "## Runoff lags\n",
    "markerline, stemlines, baseline = ax.stem(s, terminus_lag_offset+ lag_scaling*np.array(terminus_lag_amax),\n",
    "                                          bottom=terminus_lag_offset, basefmt='k:',label='Runoff lags',\n",
    "                                          markerfmt='1')\n",
    "plt.setp(stemlines, 'color', qual_colors(4))\n",
    "plt.setp(markerline, 'color', qual_colors(4))\n",
    "\n",
    "## add velocity\n",
    "ax.plot(s, velocity_offset+scaling*np.array(scaled_speed - np.mean(scaled_speed)), color=qual_colors(17), label='Mean speed', lw=2.5)\n",
    "\n",
    "\n",
    "ax.plot(xvals, bed_vals, color='saddlebrown', lw=2.0)\n",
    "ax.plot(xvals, surface_vals, color='darkgrey', lw=2.0)\n",
    "plt.fill_between(xvals, surface_vals, bed_vals, color='darkgrey', alpha=0.5)\n",
    "plt.fill_between(xvals, bed_vals, y2=-1300, color='saddlebrown', alpha=0.5, hatch='/')\n",
    "ax.set(xlim=(25, 0), ylim=(-1300, 16000), aspect=0.002, \n",
    "        xlabel='Upstream distance [km]',\n",
    "        yticks=(-1000, 0, 1000, \n",
    "                velocity_offset+scaling*scaled_ticks[1],\n",
    "                terminus_lag_offset,\n",
    "                terminus_offset,\n",
    "                runoff_lag_offset,\n",
    "                runoff_offset, \n",
    "                smb_lag_offset,\n",
    "                smb_offset),\n",
    "        yticklabels=('-1000', '0', '1000 m a.s.l.', \n",
    "                     'Surface speed',\n",
    "                     'Terminus lag',\n",
    "                     'Terminus xcorr',\n",
    "                     'Runoff lag',\n",
    "                     'Runoff xcorr',\n",
    "                     'SMB lag',\n",
    "                     'SMB xcorr'))\n",
    "minor_locator = ticker.FixedLocator([velocity_offset+scaling*scaled_ticks[0], velocity_offset+scaling*scaled_ticks[2],\n",
    "                                     terminus_lag_offset+365,\n",
    "                              terminus_offset-0.5*scaling, terminus_offset+0.5*scaling, \n",
    "                              runoff_lag_offset+365,\n",
    "                              runoff_offset-0.5*scaling, runoff_offset+0.5*scaling,\n",
    "                              smb_lag_offset+365,\n",
    "                              smb_offset-0.5*scaling, smb_offset+0.5*scaling])\n",
    "minor_formatter = ticker.FixedFormatter([4, '8 km/a',\n",
    "                                         '365 d',\n",
    "                                          -0.5, 0.5, \n",
    "                                         '365 d',\n",
    "                                        -0.5, 0.5,\n",
    "                                        '365 d',\n",
    "                                        -0.5, 0.5])\n",
    "ax.yaxis.set_minor_locator(minor_locator)\n",
    "ax.yaxis.set_minor_formatter(minor_formatter)\n",
    "ax.tick_params(axis='both', which='major', length=7)\n",
    "ax.tick_params(axis='y', which='minor', length=4)\n",
    "ax.get_yticklabels()[3].set_color(qual_colors(17)) # match the ticks to the plotted color\n",
    "for j in (4,5):\n",
    "    ax.get_yticklabels()[j].set_color(qual_colors(4)) \n",
    "for j in (6,7):\n",
    "    ax.get_yticklabels()[j].set_color(qual_colors(2))\n",
    "for j in (8,9):\n",
    "    ax.get_yticklabels()[j].set_color(qual_colors(0))\n",
    "\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.savefig('/Users/Lizz/Desktop/{}-helheim-along_flow_lag_stack.png'.format(datetime.date.today()), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
