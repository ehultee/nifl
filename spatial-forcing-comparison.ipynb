{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing spatial pattern of velocity response to forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the `iceutils` package (Bryan Riel) to invert continuous time-varying surface velocity fields on Helheim Glacier.  We'll then process several observational datasets (gathered by Denis Felikson) using the `nifl` module (Lizz Ultee) and compare time series of these variables against surface velocity at several points.  Finally, we'll visualize spatial differences in the relationship between surface velocity and each hypothesised forcing variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "from scipy import interpolate\n",
    "import pyproj as pyproj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import iceutils as ice\n",
    "import nifl_helper as nifl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define where the necessary data lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowline_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Felikson-flowlines/netcdfs/glaciera199.nc'\n",
    "velocity_fpath='/Users/lizz/Documents/GitHub/Data_unsynced/Gld-Stack/'\n",
    "gl_bed_fpath ='/Users/lizz/Documents/GitHub/Data_unsynced/BedMachine-Greenland/BedMachineGreenland-2017-09-20.nc'\n",
    "gl_smb_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/HIRHAM5-SMB/DMI-HIRHAM5_GL2_ERAI_1980_2016_SMB_MM.nc'\n",
    "catchment_smb_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Helheim-processed/HIRHAM_integrated_SMB.csv'\n",
    "runoff_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Helheim-processed/RACMO2_3p2_Helheimgletscher_runoff_1958-2017.csv'\n",
    "termini_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Helheim-processed/HLM_terminus_widthAVE.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the domain of analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse along flowlines defined by Denis Felikson in his previous work, saved and shared as NetCDF files.  The flowlines are numbered 01-10 across the terminus; flowline 05 is close to the middle.  Note that Helheim Glacier has two large branches.  For now we'll study the main trunk, `glaciera199.nc`.  The more southerly trunk is `glacierb199.nc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncfile = Dataset(flowline_fpath, 'r')\n",
    "xh = ncfile['flowline05'].variables['x'][:]\n",
    "yh = ncfile['flowline05'].variables['y'][:]\n",
    "# s = ncfile['flowline05']['geometry']['surface']['GIMP']['nominal'].variables['h'][:] # GIMP DEM\n",
    "# b = ncfile['flowline05']['geometry']['bed']['BedMachine']['nominal'].variables['h'][:] # BedMachine v3\n",
    "# dh = ncfile['flowline05']['dh']['GIMP-Arctic']['nominal']['dh'][:]\n",
    "ncfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define points at which to extract\n",
    "upstream_max = 500 # index of last xh,yh within given distance of terminus--pts roughly 50m apart\n",
    "xys = [(xh[i], yh[i]) for i in range(0, upstream_max, 25)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and invert velocity observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up combined hdf5 stack\n",
    "hel_stack = ice.MagStack(files=[velocity_fpath+'vx.h5', velocity_fpath+'vy.h5'])\n",
    "data_key = 'igram' # B. Riel convention for access to datasets in hdf5 stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evenly spaced time array for time series predictions\n",
    "t_grid = np.linspace(hel_stack.tdec[0], hel_stack.tdec[-1], 1000)\n",
    "\n",
    "# First convert the time vectors to a list of datetime\n",
    "dates = ice.tdec2datestr(hel_stack.tdec, returndate=True)\n",
    "dates_grid = ice.tdec2datestr(t_grid, returndate=True)\n",
    "\n",
    "# Build the collection\n",
    "collection = nifl.build_collection(dates)\n",
    "\n",
    "# Construct a priori covariance\n",
    "Cm = nifl.computeCm(collection)\n",
    "iCm = np.linalg.inv(Cm)\n",
    "\n",
    "# Instantiate a model for inversion\n",
    "model = ice.tseries.Model(dates, collection=collection)\n",
    "\n",
    "# Instantiate a model for prediction\n",
    "model_pred = ice.tseries.Model(dates_grid, collection=collection)\n",
    "\n",
    "## Access the design matrix for plotting\n",
    "G = model.G\n",
    "\n",
    "# Create lasso regression solver that does the following:\n",
    "# i) Uses an a priori covariance matrix for damping out the B-splines\n",
    "# ii) Uses sparsity-enforcing regularization (lasso) on the integrated B-splines\n",
    "solver = ice.tseries.select_solver('lasso', reg_indices=model.itransient, penalty=0.05,\n",
    "                                   rw_iter=1, regMat=iCm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are set up with our data and machinery, we'll ask the inversion to make us a continuous time series of velocity at each point we wish to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for xy in xys:\n",
    "    pred, st, lt = nifl.VSeriesAtPoint(xy, vel_stack=hel_stack, collection=collection, \n",
    "                                  model=model, model_pred=model_pred, solver=solver, \n",
    "                                  t_grid=t_grid, sigma=1.5, data_key='igram')\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bed topography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly we will use this for plotting and for defining a standard coordinate system.  However, future analyses could combine bed topography with calving position or other variables to analyse effect on surface velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in and interpolate BedMachine topography\n",
    "fh = Dataset(gl_bed_fpath, mode='r')\n",
    "xx = fh.variables['x'][:].copy() #x-coord (polar stereo (70, 45))\n",
    "yy = fh.variables['y'][:].copy() #y-coord\n",
    "s_raw = fh.variables['surface'][:].copy() #surface elevation\n",
    "h_raw=fh.variables['thickness'][:].copy() # Gridded thickness\n",
    "b_raw = fh.variables['bed'][:].copy() # bed topo\n",
    "thick_mask = fh.variables['mask'][:].copy()\n",
    "ss = np.ma.masked_where(thick_mask !=2, s_raw)#mask values: 0=ocean, 1=ice-free land, 2=grounded ice, 3=floating ice, 4=non-Greenland land\n",
    "hh = np.ma.masked_where(thick_mask !=2, h_raw) \n",
    "bb = b_raw #don't mask, to allow bed sampling from modern bathymetry (was subglacial in ~2006)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpolate in area of Helheim\n",
    "xl, xr = 6100, 6600\n",
    "yt, yb = 12700, 13100\n",
    "x_hel = xx[xl:xr]\n",
    "y_hel = yy[yt:yb]\n",
    "s_hel = ss[yt:yb, xl:xr]\n",
    "b_hel = bb[yt:yb, xl:xr]\n",
    "S_helheim = interpolate.RectBivariateSpline(x_hel, y_hel[::-1], s_hel.T[::,::-1]) #interpolating surface elevation provided\n",
    "B_helheim = interpolate.RectBivariateSpline(x_hel, y_hel[::-1], b_hel.T[::,::-1]) #interpolating surface elevation provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surface mass balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in surface mass balance from HIRHAM, cut out an area around Helheim, re-project it to Polar Stereographic North coordinates (to match our other data), and interpolate 2D fields for each year so that we can select surface mass balance at our points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load in HIRHAM\n",
    "fh2 = Dataset(gl_smb_fpath, mode='r')\n",
    "x_lon = fh2.variables['lon'][:].copy() #x-coord (latlon)\n",
    "y_lat = fh2.variables['lat'][:].copy() #y-coord (latlon)\n",
    "ts = fh2.variables['time'][:].copy()\n",
    "smb_raw = fh2.variables['smb'][:].copy()\n",
    "fh2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select Helheim\n",
    "xl1, xr1 = 190, 260\n",
    "yt1, yb1 = 345, 405\n",
    "x_lon_h = x_lon[yt1:yb1, xl1:xr1]\n",
    "y_lat_h = y_lat[yt1:yb1, xl1:xr1]\n",
    "\n",
    "wgs84 = pyproj.Proj(\"+init=EPSG:4326\") # LatLon with WGS84 datum used by HIRHAM\n",
    "psn_gl = pyproj.Proj(\"+init=epsg:3413\") # Polar Stereographic North used by BedMachine, Felikson lines, ...\n",
    "xs, ys = pyproj.transform(wgs84, psn_gl, x_lon_h, y_lat_h)\n",
    "Xmat, Ymat = np.meshgrid(x_hel, y_hel) # BedMachine coords from helheim-profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Timeslice-specific SMB functions 2006-2014\n",
    "SMB_dict = {} #set up a dictionary of surface mass balance fields indexed by year\n",
    "time_indices = range(311, 444) # go from Jan 2006 to Dec 2016 in monthly series\n",
    "smb_dates = pd.date_range(start='2006-01-01', end='2016-12-01', periods=len(time_indices))\n",
    "smb_d = [d.utctimetuple() for d in smb_dates]\n",
    "dates_interp = [ice.timeutils.datestr2tdec(d[0], d[1], d[2]) for d in smb_d]\n",
    "for t,d in zip(time_indices, smb_dates):\n",
    "    smb_t = smb_raw[t][0][::-1, ::][yt1:yb1, xl1:xr1]\n",
    "    regridded_smb_t = interpolate.griddata((xs.ravel(), ys.ravel()), smb_t.ravel(), (Xmat, Ymat), method='nearest')\n",
    "    SMB_dict[d] = interpolate.interp2d(x_hel, y_hel, regridded_smb_t, kind='linear')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the normalized cross-correlation between single-point SMB and surface velocity at the same point.  We will draw on the inverted velocity series saved in `preds` above.  We save the value of the maximum normalized cross-correlation, and the value in days of the lag where it occurs, to compare with other variables later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smb_corr_max = []\n",
    "smb_lag_max = []\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = nifl.SmbXcorr(xy, smb_dictionary=SMB_dict, smb_dates=smb_dates, \n",
    "                              velocity_pred=pred, t_grid=t_grid, diff=1, normalize=True)\n",
    "    smb_corr_max.append(max(corr))\n",
    "    smb_lag_max.append(lags[np.argmax(corr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catchment-integrated SMB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in a 1D timeseries of surface mass balance integrated over the whole Helheim catchment, with catchment defined following Mankoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smb = pd.read_csv(catchment_smb_fpath, parse_dates=[0])\n",
    "smb_d = [d.utctimetuple() for d in smb['Date']]\n",
    "smb_d_interp = [ice.timeutils.datestr2tdec(d[0], d[1], d[2]) for d in smb_d]\n",
    "smb_func = interpolate.interp1d(smb_d_interp, smb['SMB_int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the normalized cross-correlation between catchment-integrated SMB and surface velocity at each same point.  Again we save the value of the maximum normalized cross-correlation, and the value in days of the lag where it occurs, to compare with other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smb_corr_max = []\n",
    "smb_lag_max = []\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = nifl.RunoffXcorr(xy, runoff_func=smb_func, runoff_dates=smb_d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, diff=1, normalize=True)\n",
    "    smb_corr_max.append(max(corr))\n",
    "    smb_lag_max.append(lags[np.argmax(corr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import monthly runoff from the RACMO model, integrated over the Helheim catchment and shared as a CSV by Denis Felikson.  Because this data is catchment-integrated, we interpolate a single 1D time series that will be used at all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff = np.loadtxt(runoff_fpath, delimiter=',') \n",
    "rnf = runoff[runoff[:,0]>=2006] # trim values from before the start of the velocity series\n",
    "rf = rnf[rnf[:,0]<=2016] #trim values after the end of the velocity series\n",
    "\n",
    "runoff_dates = pd.date_range(start='2006-01-01', end='2016-12-01', periods=len(rf))\n",
    "runoff_d = [d.utctimetuple() for d in runoff_dates]\n",
    "d_interp = [ice.timeutils.datestr2tdec(d[0], d[1], d[2]) for d in runoff_d]\n",
    "runoff_func = interpolate.interp1d(d_interp, rf[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the normalized cross-correlation between catchment-integrated runoff and surface velocity at each same point.  Again we save the value of the maximum normalized cross-correlation, and the value in days of the lag where it occurs, to compare with other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff_corr_max = []\n",
    "runoff_lag_max = []\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = nifl.RunoffXcorr(xy, runoff_func=runoff_func, runoff_dates=d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, diff=1, normalize=True)\n",
    "    runoff_corr_max.append(max(corr))\n",
    "    runoff_lag_max.append(lags[np.argmax(corr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminus position change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import width-averaged terminus position change processed by Leigh Stearns.  These data give terminus position in km from a baseline, so they do not need to be processed into a coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termini = pd.read_csv(termini_fpath, parse_dates=True, usecols=[0,1])\n",
    "termini['date'] = pd.to_datetime(termini['date'])\n",
    "trmn = termini.loc[termini['date'].dt.year >= 2006]\n",
    "tm = trmn.loc[trmn['date'].dt.year <=2016]\n",
    "\n",
    "termini_d = [d.utctimetuple() for d in tm['date']]\n",
    "tm_d_interp = [ice.timeutils.datestr2tdec(d[0], d[1], d[2]) for d in termini_d]\n",
    "termini_func = interpolate.interp1d(tm_d_interp, tm['term_km'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminus_corr_max = []\n",
    "terminus_lag_max = []\n",
    "for xy, pred in zip(xys, preds):\n",
    "    corr, lags, ci = nifl.RunoffXcorr(xy, runoff_func=termini_func, runoff_dates=tm_d_interp, \n",
    "                              velocity_pred=pred, t_grid=t_grid, diff=1, normalize=True)\n",
    "    terminus_corr_max.append(max(corr))\n",
    "    terminus_lag_max.append(lags[np.argmax(corr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we plot the max correlation at each point for a single variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.contourf(x_hel, y_hel, b_hel, cmap='gist_earth', alpha=0.5)\n",
    "sc = ax.scatter(np.asarray(xys)[:,0], np.asarray(xys)[:,1], c=smb_corr_max)\n",
    "cb = fig.colorbar(sc, ax=ax)\n",
    "cb.ax.set_title('Max. xcorr')\n",
    "ax.set(xlim=(270000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', ylabel='Northing [km]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the patterns of correlation and lag for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "ax1.contourf(x_hel, y_hel, b_hel, cmap='gist_earth', alpha=0.5)\n",
    "sc1 = ax1.scatter(np.asarray(xys)[:,0], np.asarray(xys)[:,1], c=smb_corr_max)\n",
    "cb1 = fig.colorbar(sc1, ax=ax1)\n",
    "cb1.ax.set_title('Max. xcorr')\n",
    "ax1.set(xlim=(270000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', ylabel='Northing [km]', title='Surface mass balance')\n",
    "ax2.contourf(x_hel, y_hel, b_hel, cmap='gist_earth', alpha=0.5)\n",
    "sc2 = ax2.scatter(np.asarray(xys)[:,0], np.asarray(xys)[:,1], c=runoff_corr_max)\n",
    "cb2 = fig.colorbar(sc2, ax=ax2)\n",
    "cb2.ax.set_title('Max. xcorr')\n",
    "ax2.set(xlim=(270000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', ylabel='Northing [km]', title='Catchment-integrated runoff')\n",
    "ax3.contourf(x_hel, y_hel, b_hel, cmap='gist_earth', alpha=0.5)\n",
    "sc3 = ax3.scatter(np.asarray(xys)[:,0], np.asarray(xys)[:,1], c=terminus_corr_max)\n",
    "cb3 = fig.colorbar(sc3, ax=ax3)\n",
    "cb3.ax.set_title('Max. xcorr')\n",
    "ax3.set(xlim=(270000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', ylabel='Northing [km]', title='Terminus position')\n",
    "plt.subplots_adjust(right=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "ax1.contourf(x_hel, y_hel, b_hel, cmap='gist_earth', alpha=0.5)\n",
    "sc1 = ax1.scatter(np.asarray(xys)[:,0], np.asarray(xys)[:,1], c=smb_lag_max)\n",
    "cb1 = fig.colorbar(sc1, ax=ax1)\n",
    "cb1.ax.set_title('Lag [days] at peak xcorr')\n",
    "ax1.set(xlim=(270000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', ylabel='Northing [km]', title='Surface mass balance')\n",
    "ax2.contourf(x_hel, y_hel, b_hel, cmap='gist_earth', alpha=0.5)\n",
    "sc2 = ax2.scatter(np.asarray(xys)[:,0], np.asarray(xys)[:,1], c=runoff_lag_max)\n",
    "cb2 = fig.colorbar(sc2, ax=ax2)\n",
    "cb2.ax.set_title('Lag [days] at peak xcorr')\n",
    "ax2.set(xlim=(270000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', ylabel='Northing [km]', title='Catchment-integrated runoff')\n",
    "ax3.contourf(x_hel, y_hel, b_hel, cmap='gist_earth', alpha=0.5)\n",
    "sc3 = ax3.scatter(np.asarray(xys)[:,0], np.asarray(xys)[:,1], c=terminus_lag_max)\n",
    "cb3 = fig.colorbar(sc3, ax=ax3)\n",
    "cb3.ax.set_title('Lag [days] at peak xcorr')\n",
    "ax3.set(xlim=(270000, 320000), xticks=(280000, 300000, 320000), \n",
    "      ylim=(-2590000, -2550000), yticks=(-2590000, -2570000, -2550000), \n",
    "       xticklabels=('280', '300', '320'), yticklabels=('-2590', '-2570', '-2550'),\n",
    "      xlabel='Easting [km]', ylabel='Northing [km]', title='Terminus position')\n",
    "plt.subplots_adjust(right=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual chunks to compare changing seasonal cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break signals into annual subsets and compute the cross-correlation signal for each single year of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_annual_corrs = []\n",
    "rf_annual_lags = []\n",
    "rf_annual_ci = []\n",
    "\n",
    "point_to_plot =5\n",
    "date_chks = range(2009, 2017)\n",
    "for i in range(len(date_chks)-1):\n",
    "    snippet = rf[rf[:,0]>=date_chks[i]]\n",
    "    snpt = snippet[snippet[:,0]<date_chks[i+1]]\n",
    "    d_chk = [d for d in d_interp if (d>=date_chks[i] and d<=date_chks[i+1])]\n",
    "#     t_chk = np.asarray([t for t in t_grid if (t>=date_chks[i] and t<date_chks[i+1])])\n",
    "#     t_chk = t_grid[t_grid>=date_chks[i]]\n",
    "#     snpt_func = interpolate.interp1d(d_chk, snpt[:,2])\n",
    "    corr, lags, ci = nifl.Xcorr1D(xys[point_to_plot], series_func=runoff_func, series_dates=d_interp, \n",
    "                              velocity_pred=preds[point_to_plot], t_grid=t_grid, t_limits=(date_chks[i], date_chks[i+1]),\n",
    "                                  diff=1, normalize=True)\n",
    "    rf_annual_corrs.append(corr)\n",
    "    rf_annual_lags.append(lags)\n",
    "    rf_annual_ci.append(ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(len(rf_annual_corrs))\n",
    "# for j in range(len(rf_annual_corrs)):\n",
    "#     axs[j].plot(rf_annual_lags[j], rf_annual_corrs[j])\n",
    "#     axs[j].plot(rf_annual_lags[j], rf_annual_ci[j], ls=':', color='k')\n",
    "#     axs[j].plot(rf_annual_lags[j], -1*np.array(rf_annual_ci[j]), ls=':', color='k')\n",
    "\n",
    "for j in range(len(rf_annual_corrs)):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.axvline(x=0, ls='-', color='k', alpha=0.5)\n",
    "    ax.axhline(y=0, ls='-', color='k', alpha=0.5)\n",
    "    ax.plot(rf_annual_lags[j], rf_annual_corrs[j])\n",
    "    ax.plot(rf_annual_lags[j], rf_annual_ci[j], ls=':', color='k')\n",
    "    ax.plot(rf_annual_lags[j], -1*np.array(rf_annual_ci[j]), ls=':', color='k')\n",
    "    ax.set(ylim=(-1,1), title='Xcorr runoff-vel, {}'.format(date_chks[j]), \n",
    "           xlabel='Lag [days]', ylabel='xcorr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with the overall signal from the full period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, lags, ci = nifl.Xcorr1D(xys[5], series_func=runoff_func, series_dates=d_interp, \n",
    "                          velocity_pred=preds[5], t_grid=t_grid, t_limits=(2009,2017), diff=1, normalize=True)\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.axvline(x=0, ls='-', color='k', alpha=0.5)\n",
    "ax.axhline(y=0, ls='-', color='k', alpha=0.5)\n",
    "ax.plot(lags, corr)\n",
    "ax.plot(lags, ci, ls=':', color='k')\n",
    "ax.plot(lags, -1*np.array(ci), ls=':', color='k')\n",
    "ax.set(ylim=(-1,1), title='Xcorr runoff-vel, 2009-2017', xlabel='Lag [days]', ylabel='xcorr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
